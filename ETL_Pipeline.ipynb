{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP99Hdtj82pL8XK/Tx79oi/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ETL Pipeline Overview\n",
        "Extract: Read raw data from Azure Delta Lake Storage (ADLS Gen2).\n",
        "\n",
        "Transform: Process data using Databricks and Delta Lake.\n",
        "\n",
        "Load: Store the transformed data in Azure Synapse Analytics\n",
        "\n",
        "Prerequisites\n",
        "Azure Data Lake Storage (ADLS) Gen2: Stores raw data.\n",
        "\n",
        "Databricks Cluster: Runs PySpark jobs.\n",
        "\n",
        "Delta Lake: Manages ACID transactions.\n",
        "\n",
        "Azure Synapse Analytics: Stores the final data.\n",
        "\n",
        "Azure Service Principal: Securely connects Databricks to ADLS and Synapse.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_BhemVIqnuez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Required Libraries\n",
        "# Ensure your Databricks cluster has the following libraries installed:\n",
        "\n",
        "# Azure Storage SDK (azure-storage)\n",
        "\n",
        "# JDBC Driver for Synapse (com.microsoft.sqlserver:mssql-jdbc:9.4.0.jre8)"
      ],
      "metadata": {
        "id": "4D8A3rWCn8L6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Configure Access to ADLS and Synapse\n",
        "\n"
      ],
      "metadata": {
        "id": "t06iY8_-oA46"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PjHgvSOnefV"
      },
      "outputs": [],
      "source": [
        "# Databricks Configurations\n",
        "spark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\n",
        "spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\",\n",
        "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<service-principal-client-id>\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", \"<service-principal-secret>\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\",\n",
        "               \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Extract Data from Azure Delta Lake"
      ],
      "metadata": {
        "id": "zcEaT2uFoGVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read raw data from Azure Data Lake Storage\n",
        "input_path = \"abfss://<container>@<storage-account>.dfs.core.windows.net/raw/data.json\"\n",
        "\n",
        "df = spark.read.format(\"json\").load(input_path)\n",
        "\n",
        "df.show(5)  # Display sample records\n"
      ],
      "metadata": {
        "id": "16J3ETDhoFsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data Transformation Using Delta Lake"
      ],
      "metadata": {
        "id": "9rXq05xjoM8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Clean and transform data\n",
        "df_transformed = (df\n",
        "    .withColumn(\"cleaned_date\", to_date(col(\"timestamp_column\")))  # Convert to date format\n",
        "    .filter(col(\"status\") == \"active\")  # Keep only active records\n",
        ")\n",
        "\n",
        "# Write to Delta Table\n",
        "delta_table_path = \"abfss://<container>@<storage-account>.dfs.core.windows.net/delta/processed_data\"\n",
        "\n",
        "df_transformed.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
      ],
      "metadata": {
        "id": "0tornmUnoMGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Load Transformed Data into Azure Synapse"
      ],
      "metadata": {
        "id": "eAMw5VHioQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synapse_jdbc_url = \"jdbc:sqlserver://<synapse-server>.database.windows.net:1433;database=<synapse-database>;user=<synapse-user>;password=<synapse-password>;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
        "\n",
        "synapse_table = \"dbo.ProcessedData\"\n",
        "\n",
        "df_transformed.write \\\n",
        "    .format(\"com.databricks.spark.sqldw\") \\\n",
        "    .option(\"url\", synapse_jdbc_url) \\\n",
        "    .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n",
        "    .option(\"dbtable\", synapse_table) \\\n",
        "    .option(\"tempdir\", \"abfss://<container>@<storage-account>.dfs.core.windows.net/tempdir/\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()\n"
      ],
      "metadata": {
        "id": "kpCat1OIoQyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Summary\n",
        "Extract: Reads raw JSON data from ADLS Gen2.\n",
        "\n",
        "Transform: Cleans and processes the data using Delta Lake.\n",
        "\n",
        "Load: Writes the processed data to Azure Synapse Analytics."
      ],
      "metadata": {
        "id": "NT8RGifnoTsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k6_62NlanqeS"
      }
    }
  ]
}